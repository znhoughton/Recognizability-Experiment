% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{Psycholinguistics, holistic storage, language processing, lexical processing, phonological processing}
\usepackage{csquotes}
\usepackage[section]{placeins}
\usepackage{pdfcomment}
\usepackage{gb4e} \noautomath
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={The effects of frequency and predictability on the recognition of up in English verb+up collocations.},
  pdfauthor={Zachary Houghton1, Jungah Lee2, Casey Felton1, Georgia Zellou1, \& Emily Morgan1},
  pdflang={en-EN},
  pdfkeywords={Psycholinguistics, holistic storage, language processing, lexical processing, phonological processing},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{The effects of frequency and predictability on the recognition of \emph{up} in English verb+\emph{up} collocations.}
\author{Zachary Houghton\textsuperscript{1}, Jungah Lee\textsuperscript{2}, Casey Felton\textsuperscript{1}, Georgia Zellou\textsuperscript{1}, \& Emily Morgan\textsuperscript{1}}
\date{}


\shorttitle{Effects of frequency and predictability on the recognition of \emph{up}.}

\authornote{

Correspondence concerning this article should be addressed to Zachary Houghton. E-mail: \href{mailto:znhoughton@ucdavis.edu}{\nolinkurl{znhoughton@ucdavis.edu}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} University of California, Davis\\\textsuperscript{2} Chosun University}

\abstract{%
The question of what items are stored in the lexicon is one that has drawn a lot of attention in the last few decades, and while the general consensus is that a lot more is stored than we previously realized, it is still largely unclear what factors drive storage. For example, some have argued that frequency drives storage, while others have posited that predictability drives storage. Further, it is unclear what the relationship between stored multi-word items and the representation of each individual word is. For example, it is possible that stored items fuse together, losing some amount of their internal structure. The present paper examines both of these questions by looking at the recognizability of the segment \emph{up} in English V+\emph{up} phrases. We find that the time it takes to recognize \emph{up} decreases as frequency or predictability increases, but increases once again for the highest frequency or highest predictability items. Our results suggest that frequency and predictability both drive storage, and that stored items may lose some amount of their internal representation.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

When a listener hears the phrase \emph{trick or treat}, do they process it compositionally, processing each word individually before combining them into a single parse? Or do they access a single holistically stored representation of the phrase from memory? This question of to what extent larger-than-word constructions can be stored and accessed holistically is one that psycholinguists have been interested in for quite some time (e.g., Bybee, 2002, 2003; Goldberg, 2003; Nooteboom, Nooteboom, Weerman, \& Wijnen, 2002; Stemberger \& MacWhinney, 1986, 2004).

Throughout the years different theories have argued for different degrees of holistic storage, with two theories in particular dominating the field. On one hand, Chomskyan theories (e.g., Chomsky, 1965) have proposed that only necessary items (e.g., items that can't be formed compositionally) are stored. On the other hand, usage-based theories (e.g., Bybee, 2003) have proposed that some items that could in principle be formed compositionally can be stored under certain usage-based conditions, such as frequency of use.

Traditional Chomskyan theories (e.g., Chomsky, 1965) have argued that processing multi-word phrases is completely compositional: each piece is accessed individually and then combined to form the larger meaning. Some exceptions are reserved for idioms and other outliers, which can't be formed compositionally. More specifically, Chomskyan views of storage argue that whether an item is stored is determined purely by the degree of compositionality. According to these theories, if a multi-word expression can be composed from its parts then there is no need to holistically store the expression, and thus it is not stored holistically. For example since \emph{I don't know} can be processed compositionally, it would be processed by composing a representation from each of the individual words, \emph{I, don't,} and \emph{know}. On the other hand, \emph{kicked the bucket} would be stored holistically because there's very little relationship between the meaning of the individual words and the meaning of the expression (i.e., it's non-compositional).

Chomskyan theories of storage gained popularity partly because storage was thought to be a valuable resource that was taken up only by units that necessitated storage. This was perhaps influenced by the limited storage space of sophisticated computers at the time. In recent times, however, we've learned that the brain may have dramatically more space for storage than we had previously realized, with an upper bound of 10\textsuperscript{8432} bits (Wang, Liu, \& Wang, 2003). This is magnitudes larger than any current estimate of how much storage language requires.\footnote{Indeed, Mollica and Piantadosi (2019) estimated that, in terms of linguistic information, humans store only somewhere between one million and ten million bits of information, meaning that even their upper estimate is well within the capacity of the brain.} Considering this, it might not come as a surprise that there has been a rise in support for usage-based theories of holistic storage over the past few decades (Ambridge, 2020; Baayen, Schreuder, De Jong, \& Krott, 2002; Bybee, 2002, 2003; Bybee \& Hopper, 2001; Bybee \& Scheibman, 1999; Kapatsinski, 2018; Kapatsinski \& Radicke, 2009; Morgan \& Levy, 2016; Stemberger \& MacWhinney, 1986, 2004; Zang, Wang, Bai, Yan, \& Liversedge, 2024).

Usage-based theories posit that more than just non-compositional items (e.g., multi-word expressions) may be stored holistically in the lexicon, arguing that storage is driven by usage-based factors. For example, factors like frequency or predictability of the phrase may influence whether the phrase is stored holistically or not. According to these theories, in addition to idioms and non-compositional items, multi-word phrases such as \emph{I don't know} may also be stored holistically if they are used frequently enough (e.g., Ambridge, 2020; Arnon \& Snider, 2010; Hay, 2001; Kapatsinski, 2018; Kapatsinski \& Radicke, 2009; Lee \& Kapatsinski, 2015; Morgan \& Levy, 2016; Stemberger \& MacWhinney, 1986, 2004; Tomasello, 2005).

While it has become a dominant view in the field that at least some multi-word items are stored, it remains unclear what exactly the size of the units being stored is and, more so, what the factors driving storage are. Further, if multi-word representations are stored holistically, what are the consequences of this in terms of language processing?

\hypertarget{evidence-of-holistic-storage}{%
\subsection{Evidence of Holistic Storage}\label{evidence-of-holistic-storage}}

There is no shortage of evidence for holistic multi-word storage (e.g., Bybee \& Scheibman, 1999; Christiansen \& Arnon, 2017; Hay, 2001; Stemberger \& MacWhinney, 1986, 2004; Zwitserlood, 2018), especially in the phonology literature. For example, Bybee and Scheibman (1999) demonstrated that the word \emph{don't} is reduced to a larger extent in the phrase \emph{I don't know} than in other words containing \emph{don't}. In other words, the phrase \emph{I don't know} seems to have its own mental representation. If it was the case that the representation of \emph{don't} in \emph{I don't know} was the same as the representation of \emph{don't} in other contexts, then one would expect \emph{don't} to be equally reduced in both cases (which is contrary to the finding in Bybee \& Scheibman, 1999). Similarly, in Korean, certain consonants undergo tensification when they occur after the future marker -\emph{l}. The rate of this tensification is higher in high-frequency phrases than low-frequency phrases, further suggesting that high-frequency phrases may be stored holistically (Yi, 2002).

In addition to the phonology literature, the Psycholinguistics literature has also provided an abundance of evidence for multi-word storage. For example, Siyanova-Chanturia, Conklin, and Heuven (2011) demonstrated that binomial phrases (e.g., \emph{cat and dog}) are read faster in their more frequent ordering than in their less frequent ordering. Further, in a follow-up study, Morgan and Levy (2016) demonstrated that these ordering preferences for frequent binomials are not due to abstract ordering preferences (e.g., a preference for short words before long words), but are rather driven by experience with the specific binomial (i.e., how frequent each binomial ordering is), providing additional evidence that frequent binomials are stored holistically.

Further, there is also evidence of multi-word storage from the learning literature. For example, Siegelman and Arnon (2015) demonstrated that learning is facilitated by attending to the whole utterance, as opposed to attending to each individual word. Specifically, they used an artificial language paradigm to examine adult L2 learners' ability to learn grammatical gender. They found that adults learn grammatical gender better when they are presented with unsegmented utterances rather than segmented utterances. In other words, attending to the entire utterance, rather than learning to compose the utterance word-by-word, facilitated their learning. It seems plausible that the learned segments are the segments being stored, and that storing larger-than-word chunks is possibly what is facilitating the learning of grammatical gender in their study.

\hypertarget{what-drives-storage}{%
\subsection{What Drives Storage?}\label{what-drives-storage}}

Despite the evidence of multi-word holistic storage, however, it is still largely unclear what factors drive storage. Humans seem to be sensitive to a variety of statistical information, including both frequency (e.g., Bybee \& Scheibman, 1999; Kapatsinski \& Radicke, 2009; Lee \& Kapatsinski, 2015; Maye \& Gerken, 2000) and predictability (e.g, Olejarczuk, Kapatsinski, \& Baayen, 2018; Ramscar, Dye, \& Klein, 2013).

Traditionally, frequency has been assumed to be the driving factor behind multi-word storage. Indeed, most of the examples of storage given so far have been with respect to frequency. Perhaps the most famous series of studies demonstrating this were conducted by Bybee (Bybee, 2003; Bybee \& Hopper, 2001; Bybee \& Scheibman, 1999). In a series of studies, Bybee and colleagues demonstrated that a variety of words are reduced more in high-frequency contexts than low-frequency contexts (additionally see Kapatsinski, 2021 for further discussion of this). For example, in addition to the earlier examples, \emph{going to} can be reduced in the frequent future marker, \emph{gonna}, but not in the less frequent verb phrase construction describing motion (e.g., *\emph{gonna the store}, Bybee, 2003). This mirrors patterns we see on a word-level (which for the most part must be stored). For example, the reduction of vowels to schwa in English is more advanced in high-frequency words than low-frequency words (Bybee, 2003; Hooper, 1976). In other words, the fact that sound changes occur differently depending on the frequency of the word/phrase in a context suggests that they have separate representations (i.e., holistic storage).

On the other hand, predictability has not been directly examined much within the context of holistic multi-word storage. As far as the authors are aware, there is only one study directly examining the role of predictability in multi-word storage (Z. N. Houghton \& Morgan, 2023). In their study, using the maze task\footnote{In the maze task, participants are presented a sentence word-by-word. For each word in the sentence, they are presented with the target word and an ungrammatical distractor. Upon selecting the target word, the next word in the sentence is presented along with yet another ungrammatical distractor. The key measure is how long it takes for participants to select the target word.} (Boyce, Futrell, \& Levy, 2020) the authors examined whether participants were slower to select the first noun in high-predictability compound nouns in locally implausible contexts (i.e., contexts where the first noun in the compound is implausible but where the second noun eliminates the implausibility; see the below sentences) relative to low-predictability compound nouns.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \begin{description}
  \item[\textbf{High Predictability Plausible:}]
  ~~~~~~~~Jimmy spread out the peanut butter.
  \end{description}
\item
  \begin{description}
  \item[\textbf{High Predictability Implausible:}]
  ~~~~Jimmy picked up the peanut butter.
  \end{description}
\end{enumerate}

\noindent Note that in the implausible condition, the second noun always eliminates the implausibility (i.e., \emph{spread out the peanut} is implausible, but \emph{spread out the peanut butter} is not). If high-predictability compound nouns are stored holistically, participants may be able to access the full compound noun upon encountering the first noun, thus overcoming the local implausibility effect (since the second noun in the compound always eliminates the implausibility). Interestingly, they found that the first noun in the compound nouns was selected equally slower for both high and low-predictability compound nouns (relative to their plausible counterparts). That is, there was an increase in reaction time for selecting the first noun in the compound in the implausible condition (relative to the plausible condition) regardless of the predictability of the second noun in the compound noun. Their results suggest that either predictability doesn't drive the holistic storage of compound nouns or that it doesn't facilitate processing in this manner. However they noted that this may be a task effect, since they used the maze task as opposed to an eye-tracking task.

Despite the lack of direct evidence of predictability in the role of multi-word storage, however, predictability has been shown to play a crucial role in learning (Olejarczuk et al., 2018; Ramscar et al., 2013; Saffran, Aslin, \& Newport, 1996). For example, Olejarczuk et al. (2018) demonstrated that when learning new phonetic categories, learners don't just pay attention to co-occurrence rates, but actively try to predict upcoming sounds, suggesting that the learning of phonetic categories is also driven by prediction (i.e., the predictability of a given sound within a context). Further, in learning new words, Ramscar et al. (2013) demonstrated that children are sensitive to how predictable a cue is of an outcome (e.g., a high-frequency cue will be ignored if it isn't predictive of a specific outcome). Additionally, word-segmentation (i.e., learning which segments in an utterance are words) is also highly sensitive to predictability (Saffran et al., 1996). In their classic paper, Saffran et al. (1996) demonstrated that children keep track of transitional probabilities -- a measurement of predictability -- to segment the speech stream. While these are studies examining learning, not storage, the units that we learn may likely be the units we store. If predictability drives what we learn, it may also drive what we store.

Thus, the current literature presents strong evidence for the role of frequency in the storage of multi-word phrases, as well as suggests the possibility of a further influence of predictability. However, it remains unclear to what extent each of these factors drives storage and whether they interact at all with each other.

\hypertarget{processing-consequences-of-storage}{%
\subsection{Processing Consequences of Storage}\label{processing-consequences-of-storage}}

Given the evidence that a lot more may be stored than previously thought, another important question to consider is what exactly the processing consequences of storage are. Specifically, do the stored units maintain their own internal representation with respect to their component parts? For example, it is possible that the representation of high-frequency phrases, such as \emph{pick up,} retains the representations of the component parts \emph{pick} and \emph{up}. On the other hand, it is possible that the phrase lacks internal representation of the component parts, either because it was lost over time or because it was never learned -- we will revisit both of these ideas in the discussion section.

Indeed, there seems to be some evidence that multi-word phrases may not have a fully intact internal structure with respect to their component parts. For example, Kapatsinski and Radicke (2009) demonstrated that in high frequency V+\emph{up} constructions, it is harder to recognize the segment \emph{up} (with respect to medium-frequency V+\emph{up} constructions). This suggests that those items may have a holistic representation that has lost some of its internal structure. In their study, participants were given different auditory sentences and tasked with pressing a button immediately if they heard the segment \emph{up}. Interestingly, they found that recognizability of \emph{up} follows a U-shaped pattern with respect to the frequency of the phrase. That is, participants were slow to recognize \emph{up} in low frequency phrasal verbs, but for medium-high frequency phrasal verbs they were quicker to recognize \emph{up}. However, upon reaching the highest frequency words participants grew slower to recognize \emph{up} (See Figure \ref{fig:kapatsinskiplot}). Though it's important to note that the original paper does not take into account predictability. It's unclear how to account for the increase in recognition time for the highest frequency items if there is no loss of internal representation of those items.

A visualization of what a stored representation with and without internal structure may look like is presented in Figure \ref{fig:lossInternal}. The left tree represents the phrase \emph{pick up} stored with its internal structure still intact, whereas the right tree represents \emph{pick up} stored without internal structure. Note that both trees are examples of a holistically stored representation. The key difference is whether the internal structure remains intact in the holistic representation. The results from Kapatsinski and Radicke (2009) suggest that for high-frequency verb+\emph{up} collocations, their representation may be more similar to the tree on the right, since participants were slower to recognize \emph{up}. We will revisit this point in the discussion section in more detail.



\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{Figures/kapatsinskiradicke_graph} 

}

\caption{The U-shaped effect of the frequency of verb+\emph{up} constructions on the speed with which up is detected, reproduced from Kapatsinski and Radicke (2009).}\label{fig:kapatsinskiplot}
\end{figure}



\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{Figures/syntax_tree} 

}

\caption{A diagram of two ways the word \emph{pick up} could be stored. The left tree demonstrates a stored representation of \emph{pick up}, where the internal structure is still intact. The right tree demonstrates a holistically stored unit, where there is a loss of internal structure. Note that these are stored structures, as opposed to a compositional representation of \emph{pick up} which would be comprised of the individual representations \emph{pick} and \emph{up}.}\label{fig:lossInternal}
\end{figure}

It's worth noting that in the case of phrasal verbs like \emph{pick up}, it can't be the case that the entire internal representation is lost because it is possible to syntactically alternate it (e.g., \emph{pick up the cup} vs \emph{pick the cup up}). However, it is possible that semantic or lemma information is lost in the holistic representation. In other words, loss of internal representation may happen at different levels as opposed to being an all-or-nothing process.

Additionally, there is also evidence from the word-recognition literature that some stored words may also lose some of their internal structure as well. For example, Healy (1976) examined participants' ability to recognize letters in various words. He found that people were worse at recognizing the letter \emph{t} in \emph{the} than in other lower frequency words, which suggests that even words can develop a representation separate from their component pieces (in this case, the component parts being letters instead of words). If it is the case that \emph{the} is recognized as a composition of its parts, then it's unclear how to account for these results (c.f., Kapatsinski \& Radicke, 2009, who suggested that one explanation is that people don't fixate as long on high-frequency and function words, of which \emph{the} is both).

On the other hand, there is a necessary temporal linearity to speech, so listeners receive the information for some of the component parts before the entire phrase. For example, the listener necessarily hears \emph{pick} before \emph{pick up}. It seems unlikely that a listener would process \emph{pick up} without having processed \emph{pick} at all. Thus if holistically stored phrases do lose the representation of their component parts, it's unclear what exactly the relationship with processing is. One such possibility that was put forth by both Kapatsinski and Radicke (2009) and Healy (1976) is that during processing, the holistic representations compete with the representations of the individual parts for recognition. In other words: for high frequency phrases, hearing \emph{pick} may be enough to predict that the speaker intends to say \emph{pick up}. The listener may then process \emph{pick up} before actually hearing \emph{up} (thus explaining the increase in recognition times for \emph{up} in the highest frequency items). In other words, once the listener finishes processing the phrase, they move on to the rest of the utterance, even if they haven't fully processed the individual parts.\footnote{Note that competition can be implemented in other ways though, e.g., using top-down inhibition (Libben, 2005).} This is necessary to account for the results in Kapatsinski and Radicke (2009) because high-frequency phrases are still processed more quickly than lower frequency phrases. If accessing the holistic representation facilitates the accessing of the individual parts (as predicted by the IA model, James L. McClelland \& Rumelhart, 1981), then we would expect to see a decrease in recognition times for the component parts. However, an increase in recognition times suggests there is competition for recognition between the holistic representation and the representations of the individual parts.

\hypertarget{present-study}{%
\subsection{Present Study}\label{present-study}}

The present study examines the factors that drive storage and the processing consequences of storage by extending Kapatsinski and Radicke (2009) to look at the effects of both frequency, predictability, and their interaction on the processing of V+\emph{up} phrases. Similar to Kapatsinski and Radicke (2009), participants are tasked with pressing a button once they hear the segment \emph{up} (which in our study occurs either as a particle within verb phrases, e.g., \emph{pick up}, or part of a word, e.g., \emph{puppet}), but in our case the stimuli varied in both frequency and predictability. Since frequency effects are rather robust in the literature, we should at the very least see a negative correlation between frequency and recognition time (up to perhaps a certain point, where recognition time may increase). The effects of predictability on recognition times, however, are still relatively untested in the literature. If predictability is not a driving factor of storage, we should see only frequency effects on the recognizability of \emph{up}. On the other hand, if predictability does drive storage, we may see a loss of internal representation for high-predictability items. Further, if storage does result in a loss of internal structure, we should see similar effects to those found in Kapatsinski and Radicke (2009). Specifically, we should see a U-shaped effect, where recognition gets easier until we get to the highest frequency/predictability items, where recognizability should then become harder.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

Participants were recruited through the University of California, Davis Linguistics/Psychology Human Subjects Pool. 350 people participated in this study and were compensated in the form of SONA credit. All participants self-reported being native English speakers. Additionally, 44 participants were excluded due to an accuracy score below our threshold of 70\%, leaving a total of 306 participants for the data analysis.

\hypertarget{materials}{%
\subsection{Materials}\label{materials}}

We searched the Google \emph{n}-grams corpus (Lin et al., 2012) for the most predictable and the highest frequency phrases that matched our criteria of containing a verb immediately followed by the word \emph{up}. We operationalized predictability as the odds ratio of the probability of \emph{up} occurring immediately after the verb to the probability of any other word occurring (Equation \eqref{eq:logOdds}):

\begin{equation}
\label{eq:logOdds}
\frac{\mathrm{count(\textit{Verb+up})}}{\mathrm{count(\textit{Verb})} - \mathrm{count(\textit{Verb+up})}} 
\end{equation}

In non-mathematical terms, the above equation quantifies how likely \emph{up} is to follow after the verb relative to every other word that could follow. For example, the odds ratio of \emph{pick up} would be the number of times the entire verb phrase occurs -- \emph{pick up} -- divided by the number of times the verb -- \emph{pick} -- occurs without \emph{up} following it.

For the purposes of the present study, we gathered a variety of phrases that varied in both their predictability and frequency and their combination. In order to do this, we extracted the 50 most frequent Verb+\emph{up} items and the 50 most predictable ones. Next, we selected 100 more by randomly sampling from the remaining items. In order to ensure stable predictability estimates we eliminated words that a college-aged speaker wouldn't have heard more than 10 times.\footnote{Levy, Fedorenko, Breen, and Gibson (2012) extrapolated that the average college-aged speaker has heard about 350 million words in their lifetime. Thus we excluded items that had a frequency smaller than 10 per 350 million.} We then visually inspected the data to confirm that our data spanned across both the frequency and predictability continuum. This distribution is presented in Figure \ref{fig:stimplot2} below.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{write-up_files/figure-latex/stimplot2-1} 

}

\caption{log-predictability by log-frequency (per million) plot of our items.}\label{fig:stimplot2}
\end{figure}

Some verb phrases containing \emph{up} display unique syntactic patterns. For example, see the below verb phrases:

\begin{exe} 
\ex
  \begin{xlist}
    \ex The controversy stirred up a heated debate. \\
    \ex ??The controversy stirred a heated debate up. \\
  \end{xlist}
\end{exe}

These verbs show a syntactic alternation that is not present in all verb+\emph{up} collocations (e.g., \emph{stirred up a heated debate} is fine, but \emph{stirred a heated debate up} is weird at best). It is possible that due to this syntactic alternation, phrasal verbs may be stored regardless of frequency/predictability. Thus, we additionally coded our stimuli for whether they were phrasal verbs or not. This coding was done based on whether they could syntactically alternate between having the noun within the verb phrase and having the noun immediately after the verb phrase. For example, since both \emph{pick the cat up} and \emph{pick up the cat} are grammatical, \emph{pick up} was classified as a phrasal verb. Each item was checked by two of the authors. Disagreement was easily resolved by discussion and an agreement was reached for every item.

We also searched the same corpus for words that contained the segment \emph{up} (e.g., \emph{cupcake}). In order to gather a subset of words that roughly matches the frequency range of our experimental stimuli, we extracted the 50 most frequent words, then sampled from the rest of the dataset to gather an additional 100 words. These 350 items together comprise our stimuli.

For each item, we constructed two sentences: one sentence which contained \emph{up}, and one sentence that was identical except that it didn't include the segment \emph{up.} For words, the entire word was replaced. For phrases, \emph{up} was simply deleted if possible (e.g., \emph{clean up} replaced with \emph{clean}). If this resulted in an awkward sentence, the entire phrase was replaced. An example is given below.

\begin{exe} 
\ex
  \begin{xlist}
    \ex He picked up the phone and answered the call. \\
    \ex He grabbed the phone and answered the call. \\
  \end{xlist}
\end{exe}

In summary, our stimuli were comprised of 200 Verb+\emph{up} phrases that varied in both frequency and predictability, 150 words that contained \emph{up}, and 350 filler sentences which were matched with our experimental sentences with the exception of having \emph{up} replaced.

After creating the sentences, a native English speaker then recorded each sentence in a random order to minimize any list effect. We subsequently equalized the amplitude such that every sentence was roughly the same loudness.

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

Participants were presented with audio sentences via Pavlovia (\url{https://pavlovia.org/}), a website for presenting PsychoPy experiments (Peirce et al., 2019). Each participant was presented with 3 practice trials and then 350 sentences. While we had a total of 700 sentences, participants didn't see both the filler and experimental sentence for the same item, thus they only saw half of the stimuli. The order of the sentences was random and exactly half of the sentences contained the target segment (to avoid biasing the participants towards a specific response). Participants were instructed to press a key as soon as they heard the segment \emph{up}, or to press a separate key at the end of the sentence if they did not hear the target segment in the sentence. We then recorded their reaction time of the button press. The experiment took approximately 40 minutes.

\hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

The data\footnote{The stimuli, data, and analyses scripts can all be found freely available here: \url{https://github.com/znhoughton/Recognizability-Experiment}} was analyzed using General Additive Mixed models, as implemented in the \emph{mgcv} package (Wood, 2011) within the R programming environment (R Core Team, 2023). General Additive Mixed Models are models that allow us to model our outcome variable as a combination of the predictors. GAMMs differ from generalized linear regression models in that they allow the predictors to be modeled as non-linear functions, similar to polynomial regression. Specifically, in a Generalized Additive Mixed Model, beta-coefficients are replaced with a smooth function, which is a combination of splines. The more splines that we include, the more wiggly our line will be. In order to avoid overfitting, GAMMs also include a penalty term, \(\lambda\), which can be modified to penalize more wiggly lines that aren't justified by the data. While the predictors are allowed to vary non-linearly, the linking function in our case was linear (i.e., response time varied linearly with the spline functions).

For all of our models, the dependent variable was the time it took for participants to react to the onset of the target segment (i.e., the time it took participants to press the button after hearing \emph{up}). For the first model, the predictors were the interaction between log-predictability and log-frequency, which was allowed to vary non-linearly, and duration of the segment, which was not allowed to vary non-linearly. Additionally, we also included random intercepts for participant, trial, and item, as well as random by-participant slopes for predictability, frequency, and trial. Our model formula is included below in Equation \eqref{eq:gammInteraction}:

\begin{equation}
\begin{aligned}
\label{eq:gammInteraction}
log(RT) & \sim ti(Predictability, Frequency) + Duration + s(participant, bs = `re\text{'}) + s(Item, bs = `re\text{'}) \\
& + s(trial, bs = `re\text{'}) + s(Predictability, Frequency, participant, bs = `re\text{'}) 
\end{aligned}
\end{equation}

We also ran an additional analysis similar to the first model, but allowing the interaction to vary for phrasal vs non-phrasal verbs. Specifically, the model is identical to the first model with the exception that the effect of the interaction term was allowed to be different for phrasal verbs and non-phrasal verbs. This was done in order to examine whether the effect of frequency and predictability was different for phrasal verbs versus non-phrasal verbs. See Equation \eqref{eq:gammPhrasalNonphrasal}:

\begin{equation}
\begin{aligned}
\label{eq:gammPhrasalNonphrasal}
log(RT) & \sim ti(Predictability, Frequency, by = PhrasalVerb) + Duration + s(participant, bs = `re\text{'}) \\ 
& + s(Item, bs = `re\text{'}) + s(trial, bs = `re\text{'}) + s(Predictability, Frequency, participant, bs = `re\text{'}) 
\end{aligned}
\end{equation}

Additionally, we ran a Generalized Additive Model with frequency, predictability, and the interaction between frequency and predictability as fixed-effects that could vary non-linearly, and duration of the segment as a fixed-effect that could not vary non-linearly. The random-effects structure for this model was identical to the previous two models. The model syntax is included below in Equation \eqref{eq:gammFull}:

\begin{equation}
\begin{aligned}
\label{eq:gammFull}
log(RT) & \sim s(Predictability) + s(Frequency) + ti (Predictability, Frequency) + Duration \\ & + s(participant, bs = `re\text{'}) + s(Item, bs = `re\text{'})  
+ s(trial, bs = `re\text{'}) \\ & + s(Predictability, Frequency, Trial, Participant, bs = `re\text{'}) 
\end{aligned}
\end{equation}

Finally, we replicated the analyses from Kapatsinski and Radicke (2009) using two Bayesian quadratic regression models (implemented in \emph{brms;} BÃ¼rkner, 2017), one which only included frequency, and one which only included predictability. For the frequency model, the fixed-effects were log-frequency and log-frequency\(^2\), along with duration. The model also included random intercepts for participant and item, and random slopes for log-frequency by participant, duration by participant, and log-frequency\(^2\) by participant.

The quadratic regression with predictability was identical to the quadratic regression with frequency, except that log-frequency was replaced with log-predictability, and log-frequency\(^2\) was replaced with log-predictability\(^2\). The random-effects were modeled without correlations between them for both models (this was done to allow the model to run faster, since we collected a large amount of data).

The model syntax for both models is included below in Equations \eqref{eq:brmsFreq} and \eqref{eq:brmsPredic}:

\begin{equation}
\begin{aligned}
\label{eq:brmsFreq}
log(RT) & \sim log(Frequency) + Duration + log(Frequency^2) \\ & + (1 + log(Frequency) + log(Frequency^2) + Duration || Participant) + (1 || Item)
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\label{eq:brmsPredic}
log(RT) & \sim  log(Predictability) + Duration + log(Predictability^2) \\ & + (1 + log(Predictability) + log(Predictability^2) + Duration || Participant) + (1 || Item)
\end{aligned}
\end{equation}

\hypertarget{results}{%
\section{Results}\label{results}}

The effect of the interaction between frequency and predictability was not significant in any of our models (see Tables \ref{tab:gamModelTab} through \ref{tab:gamModelInterTab} for the output of each model). Further, there was no significant effect of whether the verb phrase was a phrasal verb (e.g., \emph{pick up}) or not (e.g., \emph{stir up})\footnote{A BIC analysis confirmed that the model that included whether the verb phrase was a phrasal verb or not (analysis in Table \ref{tab:gamModelPhrasalNonPhrasalTab}) was not a better fit than the identical model without it (the analysis in Table \ref{tab:gamModelTab}).}. In other words, the recognition times patterned similarly regardless of whether the item was a phrasal verb or not. Additionally, our third Generalized Additive Model (eq. \ref{eq:gammFull} and Table \ref{tab:gamModelInterTab}) suggested that there was a significant main-effect of predictability.

\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:gamModelTab}Model results for the Generalized Additive Mixed Model containing only the interaction between frequency and predictability.}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{1}{c}{edf} & \multicolumn{1}{c}{Ref.df} & \multicolumn{1}{c}{F} & \multicolumn{1}{c}{p-value}\\
\midrule
te(log-predictability, log-frequency) & 5.59 & 5.73 & 1.86 & 0.090\\
s(trial) & 0.99 & 1.00 & 115.38 & <0.001\\
s(participant) & 296.00 & 305.00 & 39.74 & <0.001\\
s(item) & 175.44 & 195.00 & 10.68 & <0.001\\
s(log-predictability, log-frequency, trial, participant) & 43.00 & 306.00 & 0.46 & 0.100\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:gamModelPhrasalNonPhrasalTab}Model results for the Generalized Additive Mixed Model cotaining the interaction between frequency and predictability for phrasal vs nonphrasal verbs.}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{1}{c}{edf} & \multicolumn{1}{c}{Ref.df} & \multicolumn{1}{c}{F} & \multicolumn{1}{c}{p-value}\\
\midrule
te(log-predictability, log-frequency):Nonphrasal & 3.93 & 3.98 & 1.46 & 0.210\\
te(log-predictability, log-frequency):Phrasal & 4.07 & 4.12 & 1.27 & 0.240\\
s(trial) & 0.99 & 1.00 & 115.65 & <0.001\\
s(participant) & 295.99 & 305.00 & 39.83 & <0.001\\
s(item) & 172.59 & 191.00 & 10.94 & <0.001\\
s(log-predictability, log-frequency, trial, participant) & 42.97 & 306.00 & 0.46 & 0.100\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:gamModelInterTab}Model results for the Generalized Additive Mixed Model cotaining Frequency, Predictability, and the interaction between them.}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{1}{c}{edf} & \multicolumn{1}{c}{Ref.df} & \multicolumn{1}{c}{F} & \multicolumn{1}{c}{p-value}\\
\midrule
s(log-frequency) & 2.43 & 2.48 & 1.68 & 0.320\\
s(log-predictability) & 1.88 & 1.92 & 3.30 & 0.030\\
s(log-frequency*log-predictability) & 0.00 & 0.00 & 0.05 & 0.990\\
s(participant) & 296.33 & 305.00 & 37.57 & <0.001\\
s(item) & 176.42 & 196.00 & 10.64 & <0.001\\
s(log-pred., log-freq., log-freq.:log-pred., participant) & 0.02 & 306.00 & 0.00 & 0.750\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

Given these results, we ran a follow-up Bayesian quadratic regression model to further examine the effects. Since the Generalized Additive Model suggested that there was no significant interaction between frequency and predictability, we left out the interaction term from the regression model. Similar to the above Bayesian models, we also modeled the random-effects without correlations between them. Equation \eqref{eq:BayesianFullModelSyntax} below presents the full model syntax:

\begin{equation}
\begin{aligned}
\label{eq:BayesianFullModelSyntax}
log(RT) & \sim  log(Frequency) + log(Predictability) + Duration + log(Frequency^2)  
+ log(Predictability^2) \\ 
& + (1 + log(Frequency) + log(Predictability) + log(Frequency^2) + log(Predictability^2) \\
& + Duration || Participant) + (1 || Item)
\end{aligned}
\end{equation}

The results of this model are presented below in Table \ref{tab:brmsQuadraticNoInter} and visualized in Figure \ref{fig:FullQuadraticPlot}. Following Z. Houghton, Kato, Baese-Berk, and Vaughn (2024), in some cases where the confidence interval crosses zero, we also report the percentage of posterior samples greater than or less than zero. For the current model, although the confidence intervals for both quadratic terms crossed zero, nearly 97\% of the posterior samples for predictability\(^2\) were greater than zero, and nearly 93\% of the posterior samples for frequency\(^2\) were greater than zero. A plot of the posterior distribution for each coefficient is presented in Figure \ref{fig:posteriorplotFullQuadratic}. The results suggest a U-shaped effect of both frequency and predictability on recognition times. In other words, participants recognized \emph{up} faster as frequency or predictability increased, except for the most frequent or most predictable items, where participants were slower to recognize \emph{up}.

\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:brmsQuadraticNoInter}Model results for the Bayesian quadratic regression model containing fixed-effects for frequency, predictability, and their quadratics.}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{1}{c}{Estimate} & \multicolumn{1}{c}{Est.Error} & \multicolumn{1}{c}{Q2.5} & \multicolumn{1}{c}{Q97.5}\\
\midrule
Intercept & -0.102 & 0.029 & -0.161 & -0.046\\
log-frequency & 0.019 & 0.011 & -0.002 & 0.041\\
log-predictability & 0.009 & 0.011 & -0.013 & 0.032\\
duration & -0.135 & 0.098 & -0.328 & 0.057\\
log-predictability\textasciicircum{}2 & 0.003 & 0.002 & -0.000 & 0.007\\
log-frequency\textasciicircum{}2 & 0.005 & 0.004 & -0.002 & 0.012\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{write-up_files/figure-latex/posteriorplotFullQuadratic-1} 

}

\caption{Plot of the posterior distribution for the beta value of each fixed-effect in our Bayesian quadratic regression model. The y-axis contains the different fixed-effects and the x-axis contains the posterior distribution of beta values for the corresponding fixed-effect.}\label{fig:posteriorplotFullQuadratic}
\end{figure}



\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{write-up_files/figure-latex/FullQuadraticPlot-1} 

}

\caption{Visualization of the model results from Table \ref{tab:brmsQuadraticNoInter} for frequency (top) and predictability (bottom). Frequencies are per million.}\label{fig:FullQuadraticPlot}
\end{figure}

Finally tables \ref{tab:brmsFreq} and \ref{tab:brmsPredic} present the results for the quadratic regression models including only frequency and frequency\(^2\) as well as the quadratic regression model including only predictability and predictability\(^2\) respectively:



\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:brmsFreq}Results for the Bayesian quadratic regression model containing only frequency and frequency\(^2\).}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{1}{c}{Estimate} & \multicolumn{1}{c}{Est.Error} & \multicolumn{1}{c}{Q2.5} & \multicolumn{1}{c}{Q97.5}\\
\midrule
Intercept & -0.102 & 0.025 & -0.150 & -0.054\\
log-frequency & 0.016 & 0.011 & -0.005 & 0.038\\
Duration & -0.084 & 0.098 & -0.274 & 0.108\\
log-frequency\textasciicircum{}2 & 0.006 & 0.004 & -0.001 & 0.013\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}



\begin{table}[H]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:brmsPredic}Results for the Bayesian quadratic regression model containing only predidctability and \(predictability^2\).}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{1}{c}{Estimate} & \multicolumn{1}{c}{Est.Error} & \multicolumn{1}{c}{Q2.5} & \multicolumn{1}{c}{Q97.5}\\
\midrule
Intercept & -0.102 & 0.025 & -0.150 & -0.054\\
log-predictability & 0.016 & 0.011 & -0.005 & 0.038\\
Duration & -0.084 & 0.098 & -0.274 & 0.108\\
log-predictability\textasciicircum{}2 & 0.006 & 0.004 & -0.001 & 0.013\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

While the confidence interval for the quadratic term in both models crosses zero, over 95\% of the posterior samples for log-frequency\(^2\) were greater than zero and over 96 percent of the posterior samples for log-predictability\(^2\) were greater than zero. A visualization of the posterior distributions for both models are presented in Figure \ref{fig:FreqOnlyBetaPlot} and Figure \ref{fig:PredicOnlyBetaPlot}. Further, visualizations of the model predictions are also included below in Figures \ref{fig:FreqOnlyPlot} and \ref{fig:PredicOnlyPlot}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{write-up_files/figure-latex/FreqOnlyBetaPlot-1} 

}

\caption{Plot of the posterior distribution for the beta value of each fixed-effect in our frequency-only quadratic regression model. The y-axis contains the different fixed-effects and the x-axis contains the posterior distribution of beta values for the corresponding fixed-effect.}\label{fig:FreqOnlyBetaPlot}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{write-up_files/figure-latex/PredicOnlyBetaPlot-1} 

}

\caption{Plot of the posterior distribution for the beta value of each fixed-effect in our predictability-only quadratic regression model. The y-axis contains the different fixed-effects and the x-axis contains the posterior distribution of beta values for the corresponding fixed-effect.}\label{fig:PredicOnlyBetaPlot}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{write-up_files/figure-latex/FreqOnlyPlot-1} 

}

\caption{Model predictions for the effects of frequency on reaction times for the frequency-only Bayesian quadratic model.}\label{fig:FreqOnlyPlot}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{write-up_files/figure-latex/PredicOnlyPlot-1} 

}

\caption{Model predictions for the effect of predictability on reaction times for the predictability-only models.}\label{fig:PredicOnlyPlot}
\end{figure}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

The present study examined the effects of frequency and predictability on the recognizability of the particle \emph{up} in English phrasal verbs. We found a U-shaped effect for both frequency and predictability on recognizability: as frequency/predictability increased, people were faster at recognizing \emph{up}, until reaching the highest frequency/most predictable items, where people were slower. These results suggest that the most predictable and/or highest frequency items have a lack of internal structure. We also found no meaningful differences between phrasal verbs (e.g., \emph{pick up}) and non-phrasal verbs (e.g., \emph{stir up}), suggesting that this effect is driven primarily by the statistical distribution of the input as opposed to syntactic properties.

First, our results suggest that both frequency and predictability drive storage, as we see an increase in recognition times for the highest frequency and highest predictability items. It is unclear how this result can arise without storage of the entire verb phrase, since in order for \emph{up} to be harder to recognize in some contexts than in other contexts, it must have a separate representation.

Our results also demonstrate that as frequency or predictability increases, recognition time decreases until reaching the highest frequency/predictability items where there is an increase in recognition time. Our results suggest competition between different levels (e.g., competition between the representation for a holistically stored phrase and the separate representations for its component parts). Specifically, for medium-high frequency items, since they are likely not stored, there is no holistically stored representation to compete, hence the decrease in recognition times. However, for the highest frequency items, they may have a holistically stored representation which may compete with, and even inhibit, the representations of the component parts, thus leading to an increase in recognition times.

This replicates previous findings that found competition between different levels during processing (Healy, 1976, 1994; e.g., Kapatsinski \& Radicke, 2009; Minkoff \& Raney, 2000). For example, as stated earlier, Healy (1976) found that people make more letter-detection errors in high-frequency words (e.g., \emph{the}) than in lower frequency words. Further, Minkoff and Raney (2000) found that letters are more difficult to detect in high-frequency nouns than in low-frequency nouns. Taken altogether, these results suggest that recognizing words or holistically stored phrases does not necessarily require processing them through the representations of the component parts, and in fact, processing the holistically stored phrases may make it even more difficult (via inhibition) to process the representations of the component parts.

On the other hand, rather than inhibiting the representations of the individual words, it's possible that the holistic representation and the representations of the individual words race for activation (without inhibition). For example, it's possible that for high-frequency and high-predictability items, when accessing the first word, e.g., \emph{pick}, the listener accesses the representation of the entire phrase (e.g., \emph{pick up}) immediately, before even hearing \emph{up}, and then continues on to process the next words (skipping over \emph{up}). Since the task is to respond when they hear \emph{up}, the delay in reaction time may be because they're not accessing the phonological representation of \emph{up}. Instead, they may access the semantic representation of the phrase without initially accessing the phonological representation of \emph{up}. They may then be recovering the phonological representation from the semantic representation of the phrase, causing a delay in recognition time. Indeed, this possibility was suggested by Healy (1976), who suggested that in reading once people process the meaning of a word, they move on to the next word regardless of whether they have processed each individual letter. However, while this is plausible in reading, it seems much less likely in speech processing. Since listeners receive auditory signal in a continuous stream, listeners don't have an option analogous to skipping to the next word in reading. Our results are instead more compatible with a race model with inhibition, such as the TRACE model (J. L. McClelland, Elman, \& LANGUAGE, 1984). Specifically, it seems plausible that rather than skipping the word, it is being inhibited by the holistic representation. That is, upon processing \emph{pick up}, the representations of the component parts \emph{pick} and \emph{up} are inhibited. It is still the case, however, that both explanations are compatible with our results. Thus further research is necessary to disentangle these two accounts.

However, neither of these possibilities alone -- competition between the representations, or accessing the phrase and moving on -- can account for the increase in recognition time. A high-frequency holistically stored representation with intact internal structure would show a similar decrease in recognition time for its component parts. This is because accessing the holistically stored representation, if its internal representation is intact, entails accessing the representations of the individual parts. One way to account for this is that the holistic representation may lack internal structure. For example, perhaps the increase in recognition time reflects a loss of internal structure over time. That is, it is possible that over time, more experience with the phrases results in a loss of the internal structure, or a weakening of the associations between the individual words and the representation of the phrase (as demonstrated in Figure \ref{fig:lossInternal}).

Another possible account is that, rather than being lost over time, perhaps the internal structure for the high-frequency and high-predictability items was never learned to begin with. For example, children are experts at statistical learning and use transitional probabilities to divide the continuous speech stream (Saffran et al., 1996). High predictability phrases in the present study, by definition, have higher transitional probabilities between words. Thus if children are relying on transitional probabilities to separate speech into individual words, the most predictable phrases may not be separated out of the speech stream initially.

Further, many high-frequency (e.g., \emph{set up}) and high-predictability (e.g., \emph{conjure up}) phrases have semantically vague relationships that might make it difficult to split them up on a semantic basis. It seems plausible then that maybe these phrases weren't learned as being composed of individual words initially and thus the internal structure for the holistically stored items may not have been learned. The example, \emph{trick or treat}, is a prime example of a phrase that does not seem to have a clear semantic relationship between the phrase and its component parts.

If it is the case that the internal structure for the phrase was never learned, it would explain why we see an increase in recognition times for \emph{up}: as one encounters the phrase more often, the association between the holistic representation and the words/sounds in the phrase increases. Even after one learns that \emph{pick up}, for example, is composed of two words, the holistic representation will still be more strongly associated with the phrase and continue to be activated.

Further, if the lack of internal representation is a function of our learning mechanisms, it may not be surprising that both predictability and frequency drive this lack of representation, since our brain employs both Hebbian (frequency-driven learning) and error-driven learning mechanisms (i.e., predictability-driven learning, Ashby, Ennis, \& Spiering, 2007; Kapatsinski, 2018).

Finally, we see the same U-shaped effect in both phrasal (e.g., \emph{pick up}) and non-phrasal verbs (e.g., \emph{stir up}). Phrasal verbs have a syntactic alternation that may lead to all of them being stored, regardless of whether they are frequent/predictable or not. Thus this possibility, at a glance, seems to be problematic for the interpretation of our results. Mainly, if the increase in reaction time is due to storage, then if all phrasal verbs are stored we would expect that all of the phrasal verbs were slower. However, while loss of internal representation indicates storage, storage does not necessitate a loss of internal representation. It is the combination of storage and usage that leads to loss of internal representation. Thus, the interpretation of our results holds regardless of whether phrasal verbs as a whole are stored holistically.

In summary, we demonstrate that both frequency and predictability drive the holistic storage of phrasal verbs, and these holistically stored items may compete with their component parts during lexical access. Further, we demonstrate that the most frequent and most predictable items do not have a fully intact internal representation. Future work would do well to examine if stored items are learned without internal structure or if the internal structure is lost over time as a function of experience.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

The authors would like to thank Dingyi (Penny) Pan, a graduate student in Linguistics, for reading and offering comments on a draft of this paper. The authors would also like to thank Vsevolod Kapatsinski and Zara Harmon for providing interesting discussion about the nature of competition in processing.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-ambridgeStoredAbstractionsRadical2020}{}}%
Ambridge, B. (2020). Against stored abstractions: A radical exemplar model of language acquisition. \emph{First Language}, \emph{40}(5-6), 509--559. \url{https://doi.org/10.1177/0142723719869731}

\leavevmode\vadjust pre{\hypertarget{ref-arnon2010}{}}%
Arnon, I., \& Snider, N. (2010). More than words: Frequency effects for multi-word phrases. \emph{Journal of Memory and Language}, \emph{62}(1), 67--82. \url{https://doi.org/10.1016/j.jml.2009.09.005}

\leavevmode\vadjust pre{\hypertarget{ref-ashby2007}{}}%
Ashby, F. G., Ennis, J. M., \& Spiering, B. J. (2007). A neurobiological theory of automaticity in perceptual categorization. \emph{Psychological Review}, \emph{114}(3), 632. Retrieved from \url{https://psycnet.apa.org/journals/rev/114/3/632/}

\leavevmode\vadjust pre{\hypertarget{ref-baayenDutchInflectionRules2002}{}}%
Baayen, H., Schreuder, R., De Jong, N., \& Krott, A. (2002). \emph{Dutch inflection: The rules that prove the exception} (S. Nooteboom, F. Weerman, \& F. Wijnen, Eds.). Dordrecht: Springer Netherlands. Retrieved from \url{http://link.springer.com/10.1007/978-94-010-0355-1_3}

\leavevmode\vadjust pre{\hypertarget{ref-boyceMazeMadeEasy2020}{}}%
Boyce, V., Futrell, R., \& Levy, R. P. (2020). Maze made easy: Better and easier measurement of incremental processing difficulty. \emph{Journal of Memory and Language}, \emph{111}(November 2019), 104082. \url{https://doi.org/10.1016/j.jml.2019.104082}

\leavevmode\vadjust pre{\hypertarget{ref-brms}{}}%
BÃ¼rkner, P.-C. (2017). {brms}: An {R} package for {Bayesian} multilevel models using {Stan}. \emph{Journal of Statistical Software}, \emph{80}(1), 1--28. \url{https://doi.org/10.18637/jss.v080.i01}

\leavevmode\vadjust pre{\hypertarget{ref-bybee2002}{}}%
Bybee, J. (2002). Word frequency and context of use in the lexical diffusion of phonetically conditioned sound change. \emph{Language Variation and Change}, \emph{14}(3), 261--290. \url{https://doi.org/10.1017/S0954394502143018}

\leavevmode\vadjust pre{\hypertarget{ref-bybee2003}{}}%
Bybee, J. (2003). \emph{Phonology and language use} (Vol. 94). Cambridge University Press.

\leavevmode\vadjust pre{\hypertarget{ref-bybee2001}{}}%
Bybee, J., \& Hopper, P. (2001). Introduction to frequency and the emergence of linguistic structure. \emph{Typological Studies in Language}, \emph{45}, 126.

\leavevmode\vadjust pre{\hypertarget{ref-bybee1999}{}}%
Bybee, J., \& Scheibman, J. (1999). The effect of usage on degrees of constituency: The reduction of don't in english. \emph{Linguistics}, \emph{37}(4). \url{https://doi.org/10.1515/ling.37.4.575}

\leavevmode\vadjust pre{\hypertarget{ref-chomskyAspectsTheorySyntax1965}{}}%
Chomsky, N. (1965). \emph{Aspects of the theory of syntax special technical report no. 11}. Retrieved from \url{https://ntrs.nasa.gov/api/citations/19670002070/downloads/19670002070.pdf}

\leavevmode\vadjust pre{\hypertarget{ref-christiansen2017}{}}%
Christiansen, M. H., \& Arnon, I. (2017). More than words: The role of multiword sequences in language learning and use. \emph{Topics in Cognitive Science}, \emph{9}(3), 542--551. \url{https://doi.org/10.1111/tops.12274}

\leavevmode\vadjust pre{\hypertarget{ref-goldberg2003}{}}%
Goldberg, A. E. (2003). Constructions: A new theoretical approach to language. \emph{Trends in Cognitive Sciences}, \emph{7}(5), 219--224. \url{https://doi.org/10.1016/S1364-6613(03)00080-9}

\leavevmode\vadjust pre{\hypertarget{ref-hay2001}{}}%
Hay, J. (2001). Lexical frequency in morphology: Is everything relative? \emph{Linguistics}, \emph{39}(376), 1041--1070. \url{https://doi.org/10.1515/ling.2001.041}

\leavevmode\vadjust pre{\hypertarget{ref-healy1976}{}}%
Healy, A. F. (1976). Detection errors on the word the: Evidence for reading units larger than letters. \emph{Journal of Experimental Psychology: Human Perception and Performance}, \emph{2}(2), 235. Retrieved from \url{https://psycnet.apa.org/journals/xhp/2/2/235/}

\leavevmode\vadjust pre{\hypertarget{ref-healy1994}{}}%
Healy, A. F. (1994). Letter detection: A window to unitization and other cognitive processes in reading text. \emph{Psychonomic Bulletin \& Review}, \emph{1}(3), 333--344. \url{https://doi.org/10.3758/BF03213975}

\leavevmode\vadjust pre{\hypertarget{ref-hooper1976}{}}%
Hooper, J. B. (1976). Word frequency in lexical diffusion and the source of morphophonological change. \emph{Current Progress in Historical Linguistics}, \emph{96}, 105.

\leavevmode\vadjust pre{\hypertarget{ref-houghton2023does}{}}%
Houghton, Z. N., \& Morgan, E. (2023). Does predictability drive the holistic storage of compound nouns? \emph{Proceedings of the Annual Meeting of the Cognitive Science Society}, \emph{45}.

\leavevmode\vadjust pre{\hypertarget{ref-houghtonTaskdependentConsequencesDisfluency2023}{}}%
Houghton, Z., Kato, M., Baese-Berk, M., \& Vaughn, C. (2024). Task-dependent consequences of disfluency in perception of native and non-native speech. \emph{Applied Psycholinguistics}, 1--17. \url{https://doi.org/10.1017/S0142716423000486}

\leavevmode\vadjust pre{\hypertarget{ref-kapatsinski2018}{}}%
Kapatsinski, V. (2018). \emph{Changing minds changing tools: From learning theory to language acquisition to language change}. MIT Press.

\leavevmode\vadjust pre{\hypertarget{ref-kapatsinskiHierarchicalInferenceSound2021}{}}%
Kapatsinski, V. (2021). Hierarchical inference in sound change: Words, sounds, and frequency of use. \emph{Frontiers in Psychology}, \emph{12}(August). \url{https://doi.org/10.3389/fpsyg.2021.652664}

\leavevmode\vadjust pre{\hypertarget{ref-kapatsinski2009}{}}%
Kapatsinski, V., \& Radicke, J. (2009). \emph{Frequency and the emergence of prefabs: Evidence from monitoring}. (January 2009), 499. \url{https://doi.org/10.1075/tsl.83.14kap}

\leavevmode\vadjust pre{\hypertarget{ref-lee2015}{}}%
Lee, O., \& Kapatsinski, V. (2015). \emph{Frequency effects in morphologisation of korean /n/-epenthesis}. 1--23.

\leavevmode\vadjust pre{\hypertarget{ref-levyProcessingExtraposedStructures2012}{}}%
Levy, R., Fedorenko, E., Breen, M., \& Gibson, E. (2012). The processing of extraposed structures in english. \emph{Cognition}, \emph{122}(1), 12--36. \url{https://doi.org/10.1016/j.cognition.2011.07.012}

\leavevmode\vadjust pre{\hypertarget{ref-libbenEverythingPsycholinguisticsMaterial2005}{}}%
Libben, G. (2005). Everything is psycholinguistics: Material and methodological considerations in the study of compound processing. \emph{Canadian Journal of Linguistics/Revue Canadienne de Linguistique}, \emph{50}(1-4), 267283. Retrieved from \url{https://www.cambridge.org/core/journals/canadian-journal-of-linguistics-revue-canadienne-de-linguistique/article/everything-is-psycholinguistics-material-and-methodological-considerations-in-the-study-of-compound-processing/BF01DE531A8F8305E2A664711A7C4DB3}

\leavevmode\vadjust pre{\hypertarget{ref-linSyntacticAnnotationsGoogle2012}{}}%
Lin, Y., Michel, J.-B., Lieberman, E. A., Orwant, J., Brockman, W., \& Petrov, S. (2012). \emph{Syntactic annotations for the google books ngram corpus}. 169174. Retrieved from \url{https://aclanthology.org/P12-3029.pdf}

\leavevmode\vadjust pre{\hypertarget{ref-maye2000}{}}%
Maye, J., \& Gerken, L. (2000). \emph{Learning phonemes without minimal pairs}. \emph{2}, 522533.

\leavevmode\vadjust pre{\hypertarget{ref-mcclellandTRACEModelSpeech1984}{}}%
McClelland, J. L., Elman, J. L., \& LANGUAGE, C. U. S. D. L. J. C. F. R. I. (1984). The TRACE model of speech perception. \emph{California University San Diego, La Jolla Center for Research in Language}. Retrieved from \url{https://apps.dtic.mil/sti/citations/ADA157550}

\leavevmode\vadjust pre{\hypertarget{ref-mcclellandInteractiveActivationModel1981}{}}%
McClelland, James L., \& Rumelhart, D. E. (1981). An interactive activation model of context effects in letter perception: I. An account of basic findings. \emph{Psychological Review}, \emph{88}(5), 375. Retrieved from \url{https://psycnet.apa.org/record/1981-31825-001}

\leavevmode\vadjust pre{\hypertarget{ref-minkoff2000}{}}%
Minkoff, S. R. B., \& Raney, G. E. (2000). Letter-Detection Errors in the Word The: Word Frequency Versus Syntactic Structure. \emph{Scientific Studies of Reading}, \emph{4}(1), 55--76. \url{https://doi.org/10.1207/S1532799XSSR0401_5}

\leavevmode\vadjust pre{\hypertarget{ref-mollica2019}{}}%
Mollica, F., \& Piantadosi, S. T. (2019). Humans store about 1.5 megabytes of information during language acquisition. \emph{Royal Society Open Science}, \emph{6}(3), 181393. \url{https://doi.org/10.1098/rsos.181393}

\leavevmode\vadjust pre{\hypertarget{ref-morgan2016}{}}%
Morgan, E., \& Levy, R. (2016). Frequency-dependent regularization in iterated learning. \emph{The Evolution of Language: Proceedings of the 11th International Conference (EVOLANGX11)}, (2015).

\leavevmode\vadjust pre{\hypertarget{ref-nooteboom2002}{}}%
Nooteboom, S., Nooteboom, S., Weerman, F., \& Wijnen, F. (2002). \emph{Storage and computation in the language faculty}. Springer Science \& Business Media.

\leavevmode\vadjust pre{\hypertarget{ref-olejarczuk2018}{}}%
Olejarczuk, P., Kapatsinski, V., \& Baayen, R. H. (2018). Distributional learning is error-driven: The role of surprise in the acquisition of phonetic categories. \emph{Linguistics Vanguard}, \emph{4}(s2), 1--9. \url{https://doi.org/10.1515/lingvan-2017-0020}

\leavevmode\vadjust pre{\hypertarget{ref-peircePsychoPy2ExperimentsBehavior2019}{}}%
Peirce, J., Gray, J. R., Simpson, S., MacAskill, M., HÃ¶chenberger, R., Sogo, H., \ldots{} LindelÃ¸v, J. K. (2019). PsychoPy2: Experiments in behavior made easy. \emph{Behavior Research Methods}, \emph{51}(1), 195--203. \url{https://doi.org/10.3758/s13428-018-01193-y}

\leavevmode\vadjust pre{\hypertarget{ref-Rpackage}{}}%
R Core Team. (2023). \emph{R: A language and environment for statistical computing}. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from \url{https://www.R-project.org/}

\leavevmode\vadjust pre{\hypertarget{ref-ramscar2013}{}}%
Ramscar, M., Dye, M., \& Klein, J. (2013). Children value informativity over logic in word learning. \emph{Psychological Science}, \emph{24}(6), 1017--1023. \url{https://doi.org/10.1177/0956797612460691}

\leavevmode\vadjust pre{\hypertarget{ref-saffran1996}{}}%
Saffran, J. R., Aslin, R. N., \& Newport, E. L. (1996). Statistical Learning by 8-Month-Old Infants. \emph{Science}, \emph{274}(5294), 1926--1928. \url{https://doi.org/10.1126/science.274.5294.1926}

\leavevmode\vadjust pre{\hypertarget{ref-siegelman2015}{}}%
Siegelman, N., \& Arnon, I. (2015). The advantage of starting big: Learning from unsegmented input facilitates mastery of grammatical gender in an artificial language. \emph{Journal of Memory and Language}, \emph{85}, 60--75. \url{https://doi.org/10.1016/j.jml.2015.07.003}

\leavevmode\vadjust pre{\hypertarget{ref-siyanova-chanturia2011}{}}%
Siyanova-Chanturia, A., Conklin, K., \& Heuven, W. J. B. van. (2011). Seeing a phrase {``}time and again{"} matters: The role of phrasal frequency in the processing of multiword sequences. \emph{Journal of Experimental Psychology: Learning Memory and Cognition}, \emph{37}(3), 776--784. \url{https://doi.org/10.1037/a0022531}

\leavevmode\vadjust pre{\hypertarget{ref-stemberger1986}{}}%
Stemberger, J. P., \& MacWhinney, B. (1986). Frequency and the lexical storage of regularly inflected forms. \emph{Memory \& Cognition}, \emph{14}(1), 17--26. \url{https://doi.org/10.3758/BF03209225}

\leavevmode\vadjust pre{\hypertarget{ref-stemberger2004}{}}%
Stemberger, J. P., \& MacWhinney, B. (2004). Are inflected forms stored in the lexicon. \emph{Morphology: Critical Concepts in Linguistics}, \emph{6}, 107122.

\leavevmode\vadjust pre{\hypertarget{ref-tomasello2005}{}}%
Tomasello, M. (2005). \emph{Constructing a language: A usage-based theory of language acquisition}. Harvard university press.

\leavevmode\vadjust pre{\hypertarget{ref-wang2003}{}}%
Wang, Y., Liu, D., \& Wang, Y. (2003). Discovering the Capacity of Human Memory. \emph{Brain and Mind}, \emph{4}(2), 189--198. \url{https://doi.org/10.1023/A:1025405628479}

\leavevmode\vadjust pre{\hypertarget{ref-mgcv}{}}%
Wood, S. N. (2011). \emph{Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models}. \emph{73}, 3--36.

\leavevmode\vadjust pre{\hypertarget{ref-yi2002}{}}%
Yi, B.-W. (2002). Å¬mun hyÅnsanggwa pindo hyogwa {[}the effects of frequency and phonology{]}. \emph{Han'gugÅhak}, \emph{15}, 161--183.

\leavevmode\vadjust pre{\hypertarget{ref-zangParafovealProcessingChinese2024}{}}%
Zang, C., Wang, S., Bai, X., Yan, G., \& Liversedge, S. P. (2024). Parafoveal processing of chinese four-character idioms and phrases in reading: Evidence for multi-constituent unit hypothesis. \emph{Journal of Memory and Language}, \emph{136}, 104508. \url{https://doi.org/10.1016/j.jml.2024.104508}

\leavevmode\vadjust pre{\hypertarget{ref-zwitserlood2018}{}}%
Zwitserlood, P. (2018). Processing and representation of morphological complexity in native language comprehension and production. \emph{The Construction of Words: Advances in Construction Morphology}, 583--602. \url{https://doi.org/10.1007/978-3-319-74394-3_20}

\end{CSLReferences}


\end{document}
